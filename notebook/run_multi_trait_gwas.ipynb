{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 2.4.1\n",
      "SparkUI available at http://10.151.24.211:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.28-61941242c15d\n",
      "LOGGING: writing to /Users/yanyul/Documents/repo/github/toy-example-of-hail-using-1kg/notebook/hail-20191212-1754-0.2.28-61941242c15d.log\n"
     ]
    }
   ],
   "source": [
    "import hail as hl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import os.path\n",
    "hl.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering for variants: (10961, 284)\n",
      "After filtering for variants: (8777, 284)\n"
     ]
    }
   ],
   "source": [
    "variant = hl.read_table('output/variant_qc.ht')\n",
    "mt = hl.read_matrix_table('data/1kg.mt')\n",
    "print('Before filtering for variants:', mt.count())\n",
    "mt = mt.filter_rows(hl.is_defined(variant[mt.locus, mt.alleles]))\n",
    "print('After filtering for variants:', mt.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-12 17:54:19 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 's' as type 'str' (type not specified)\n",
      "  Loading column 'trait_0' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_1' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_2' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_3' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_4' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_5' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_6' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_7' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_8' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_9' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_10' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_11' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_12' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_13' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_14' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_15' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_16' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_17' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_18' as type 'float64' (user-specified)\n",
      "  Loading column 'trait_19' as type 'float64' (user-specified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "type_dic = { f'trait_{i}' : hl.tfloat for i in range(20) }\n",
    "pheno_table = hl.import_table('output/indiv_pheno.tsv', types = type_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments go here\n",
    "Here we take two random subsets of individuals and run two multi-trait GWASs.\n",
    "The idea is to test the usage of `linear_regression_rows` in hail 0.2 in which `y` can be list of lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before running the thing\n",
    "We take a detour to create individual subsetting list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsubset = 2\n",
    "if not os.path.isfile('output/training_0.txt'): \n",
    "    def draw_subset(ntotal, ntarget, seed):\n",
    "        if ntotal < ntarget:\n",
    "            return None\n",
    "        else:\n",
    "            np.random.seed(seed)\n",
    "            return np.random.choice(ntotal, ntarget, replace = False)\n",
    "    def save_as_table(vec, f):\n",
    "        o = open(f, 'w')\n",
    "        for i in vec:\n",
    "            o.write(i + '\\n')\n",
    "        o.close()\n",
    "    indiv_list = np.array(mt.s.collect())\n",
    "    subset_size = 200\n",
    "    for i in range(nsubset):\n",
    "        subset_idx = draw_subset(indiv_list.shape[0], subset_size, seed = i)\n",
    "        training = indiv_list[subset_idx]\n",
    "        save_as_table(training, f'output/training_{i}.txt')\n",
    "        test = indiv_list[np.where(np.isin(np.array(range(indiv_list.shape[0])), subset_idx, invert = True))]\n",
    "        save_as_table(test, f'output/test_{i}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OK, we can start to work on the GWAS pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-12 17:54:21 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-12 17:54:22 Hail: INFO: Coerced sorted dataset\n"
     ]
    }
   ],
   "source": [
    "# loading phenotypes\n",
    "pheno_full_table = pd.read_csv('output/indiv_pheno.tsv', sep = '\\t')\n",
    "target_trait_list = [ f'trait_{i}' for i in range(12) ]\n",
    "my_pheno_list = []\n",
    "for i in range(nsubset):\n",
    "    pheno_set1 = pd.read_csv(f'output/training_{i}.txt', sep = '\\t', header = None)\n",
    "    pheno_subset = pheno_full_table.loc[pheno_full_table['s'].isin(pheno_set1[0].to_list())]\n",
    "    pheno_subset = pheno_subset[['s'] + target_trait_list]\n",
    "    pheno_subset_ht = hl.Table.from_pandas(pheno_subset, key = 's')\n",
    "    my_pheno_list.append(pheno_subset_ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-12 17:54:22 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 's' as type 'str' (type not specified)\n",
      "  Loading column 'covar_0' as type 'float64' (user-specified)\n",
      "  Loading column 'covar_1' as type 'float64' (user-specified)\n",
      "  Loading column 'covar_2' as type 'float64' (user-specified)\n",
      "  Loading column 'covar_3' as type 'float64' (user-specified)\n",
      "  Loading column 'covar_4' as type 'float64' (user-specified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading covariates\n",
    "type_dic = { f'covar_{i}': hl.tfloat for i in range(5) }\n",
    "covar = hl.import_table('output/indiv_covar.tsv', types = type_dic, key = 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks good, let's run GWAS\n",
    "# annotate phenotypes\n",
    "anno_pheno_expr = {\n",
    "    f'subset_{i}': my_pheno_list[i][mt.s] for i in range(len(my_pheno_list))\n",
    "}\n",
    "mt = mt.annotate_cols(**anno_pheno_expr)\n",
    "# annotate covariates\n",
    "mt = mt.annotate_cols(covariate = covar[mt.s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-12 17:54:23 Hail: WARN: 84 of 284 samples have a missing phenotype or covariate.\n",
      "2019-12-12 17:54:23 Hail: WARN: 84 of 284 samples have a missing phenotype or covariate.\n",
      "2019-12-12 17:54:23 Hail: INFO: linear_regression_rows[0]: running on 200 samples for 12 response variables y,\n",
      "    with input variable x, and 6 additional covariates...\n",
      "2019-12-12 17:54:24 Hail: INFO: linear_regression_rows[1]: running on 200 samples for 12 response variables y,\n",
      "    with input variable x, and 6 additional covariates...\n"
     ]
    }
   ],
   "source": [
    "# and run linear_regression_rows\n",
    "pheno_list = [ [ mt[f'subset_{i}'][j] for j in mt[f'subset_{i}'] ] for i in range(nsubset) ]\n",
    "pheno_names = [ [ f'subset_{i}_x_{j}' for j in mt[f'subset_{i}'] ] for i in range(nsubset) ]\n",
    "covar_list = [ mt.covariate[i] for i in list(mt.covariate.keys()) ]\n",
    "gwas_out = hl.linear_regression_rows(\n",
    "    y = pheno_list,\n",
    "    x = mt.GT.n_alt_alleles(),\n",
    "    covariates = covar_list + [1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    'phenotypes': array<array<str>> \n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh37> \n",
      "    'alleles': array<str> \n",
      "    'n': array<int32> \n",
      "    'sum_x': array<float64> \n",
      "    'y_transpose_x': array<array<float64>> \n",
      "    'beta': array<array<float64>> \n",
      "    'standard_error': array<array<float64>> \n",
      "    't_stat': array<array<float64>> \n",
      "    'p_value': array<array<float64>> \n",
      "----------------------------------------\n",
      "Key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gwas_out = gwas_out.annotate_globals(phenotypes = pheno_names)\n",
    "gwas_out.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OK, the last step is to format results into tables by trait\n",
    "Here I mostly follow routine in [here](https://github.com/Nealelab/UK_Biobank_GWAS/blob/95ac260a5d4cf9c40effff13fa33fb95ed825e2a/0.2/run_regressions.biomarkers.py#L54) and [here](https://github.com/Nealelab/UK_Biobank_GWAS/blob/95ac260a5d4cf9c40effff13fa33fb95ed825e2a/0.2/export_results.biomarkers.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-12 17:54:25 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-12 17:54:27 Hail: INFO: merging 2 files totalling 914.4K...\n",
      "2019-12-12 17:54:27 Hail: INFO: while writing:\n",
      "    output/gwas_subset_0_x_trait_0.tsv\n",
      "  merge time: 21.489ms\n",
      "2019-12-12 17:54:27 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-12 17:54:28 Hail: INFO: merging 2 files totalling 905.8K...\n",
      "2019-12-12 17:54:28 Hail: INFO: while writing:\n",
      "    output/gwas_subset_0_x_trait_1.tsv\n",
      "  merge time: 21.347ms\n",
      "2019-12-12 17:54:29 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-12 17:54:30 Hail: INFO: merging 2 files totalling 905.8K...\n",
      "2019-12-12 17:54:30 Hail: INFO: while writing:\n",
      "    output/gwas_subset_0_x_trait_2.tsv\n",
      "  merge time: 15.068ms\n",
      "2019-12-12 17:54:30 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-12 17:54:31 Hail: INFO: merging 2 files totalling 914.5K...\n",
      "2019-12-12 17:54:31 Hail: INFO: while writing:\n",
      "    output/gwas_subset_0_x_trait_3.tsv\n",
      "  merge time: 16.550ms\n",
      "2019-12-12 17:54:31 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-12 17:54:32 Hail: INFO: merging 2 files totalling 906.0K...\n",
      "2019-12-12 17:54:32 Hail: INFO: while writing:\n",
      "    output/gwas_subset_0_x_trait_4.tsv\n",
      "  merge time: 15.716ms\n",
      "2019-12-12 17:54:33 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-12 17:54:33 Hail: INFO: merging 2 files totalling 905.5K...\n",
      "2019-12-12 17:54:33 Hail: INFO: while writing:\n",
      "    output/gwas_subset_0_x_trait_5.tsv\n",
      "  merge time: 13.370ms\n",
      "2019-12-12 17:54:34 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-12 17:54:34 Hail: INFO: merging 2 files totalling 906.1K...\n",
      "2019-12-12 17:54:34 Hail: INFO: while writing:\n",
      "    output/gwas_subset_0_x_trait_6.tsv\n",
      "  merge time: 13.617ms\n",
      "2019-12-12 17:54:35 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-12 17:54:35 Hail: INFO: merging 2 files totalling 906.9K...\n",
      "2019-12-12 17:54:35 Hail: INFO: while writing:\n",
      "    output/gwas_subset_0_x_trait_7.tsv\n",
      "  merge time: 13.751ms\n",
      "2019-12-12 17:54:36 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    }
   ],
   "source": [
    "gwas_out = gwas_out.annotate( \n",
    "    variant = hl.delimit(\n",
    "        hl.array([\n",
    "            gwas_out['locus'].contig,\n",
    "            hl.str(gwas_out['locus'].position),\n",
    "            gwas_out['alleles'][0],\n",
    "            gwas_out['alleles'][1]\n",
    "        ]), \n",
    "    delimiter = ':')\n",
    ")\n",
    "gwas_out = gwas_out.key_by('variant')\n",
    "# gwas_out = gwas_out.repartition(116)\n",
    "# gwas_out = gwas_out.cache()\n",
    "\n",
    "phenotypes = gwas_out['phenotypes'].collect()[0]\n",
    "for i, subset in enumerate(phenotypes):\n",
    "    for j, trait in enumerate(subset):\n",
    "#         trait_name = trait.split('_x_')[-1]\n",
    "        ht_export = gwas_out.annotate(\n",
    "            n_complete_samples = gwas_out['n'][i],\n",
    "            AC = gwas_out['sum_x'][i],\n",
    "            ytx = gwas_out['y_transpose_x'][i][j],\n",
    "            beta = gwas_out['beta'][i][j],\n",
    "            se = gwas_out['standard_error'][i][j],\n",
    "            tstat = gwas_out['t_stat'][i][j],\n",
    "            pval = gwas_out['p_value'][i][j])\n",
    "        ht_export = ht_export.annotate(\n",
    "            AF = ht_export['AC'] / (2 * ht_export['n_complete_samples']))\n",
    "        ht_export = ht_export.annotate(\n",
    "            minor_AF = hl.cond(ht_export['AF'] <= 0.5, ht_export['AF'], 1.0 - ht_export['AF']),\n",
    "            minor_allele = hl.cond(ht_export['AF'] <= 0.5, ht_export['alleles'][1], ht_export['alleles'][0]))\n",
    "        ht_export = ht_export.annotate(\n",
    "            low_confidence_variant=ht_export['minor_AF'] < 0.001)\n",
    "        ht_export = ht_export.select(\n",
    "            'minor_allele',\n",
    "            'minor_AF',\n",
    "            'low_confidence_variant',\n",
    "            'n_complete_samples',\n",
    "            'AC',\n",
    "            'ytx',\n",
    "            'beta',\n",
    "            'se',\n",
    "            'tstat',\n",
    "            'pval')\n",
    "        ht_export.export(f'output/gwas_{trait}.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gwas_out.annotate_globals()\n",
    "# phenotypes = gwas_out['phenotypes'].collect()[0]\n",
    "# phenotypes\n",
    "# gwas_out['n'][i].show()\n",
    "# gwas_out['y_transpose_x'][i][j].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
